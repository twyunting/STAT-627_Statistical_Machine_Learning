---
title: "STAT-627 Assignment 2"
author: "Yunting Chiu"
date: "`r Sys.Date()`"
output:
  html_document: 
    number_sections: yes
    theme: cerulean
    highlight: kate
urlcolor: blue
---

```{r setup, include=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(tidymodels)
library(corrplot)
library(ggthemes)
library(discrim)
library(leaps)
```
In this assignment, I will be using [Tidymodels](https://www.tidymodels.org/) instead of base R to do coding.

# Exercise 1 (10 points)

Suppose we collect data for a group of students in a statistics class with variables $X_1$ = hours studied, $X_2$ = undergrad GPA, and $Y$ = receive an A. We fit a logistic regression and produce estimated coefficient, $\hat{\beta}_0=-6$, $\hat{\beta}_1=0.05$, $\hat{\beta}_2=1$.

The equation of logistic regression is:
$$
p(X) = \frac{e^{\beta_0 + \beta_1X_1 + … + \beta_pX_p}}{1 + e^{\beta_0 + \beta_1X_1 + … + \beta_pX_p}}
$$
a) Estimate the probability that a student who studies for 40 hours and has an undergrad GPA of $3.5$ gets an A in the class.
 
According to the question, $X_1$ is 40 hours and $X_2$ is $3.5$ so the probability will be:
$$
\hat{p}(X) = \frac{e^{-6 + 0.05X_1 + 1X_2}}{(1 + e^{-6 + 0.05X_1 + 1X_2})}
$$

`e` is an Euler's number, which is close to 2.7182818284590452353602874713527.... Using `exp()` in the console, the probability of getting an A will be 0.3775
$$
\hat{p}(X) = \frac{e^{-6 + 0.05*40 + 1*3.5}}{(1 + e^{-6 + 0.05*40 + 1*3.5})} = 0.3775407
$$
We can create a function to compute the probability, the answer is p = 37.8 %.
```{r}
prob <- function(x1,x2){
  stopifnot(is.numeric(x1) & is.numeric(x2))
  tmp <- exp(-6 + 0.05 * x1 + 1 * x2) 
  return(tmp/(1 + tmp))
}

prob(40, 3.5)
```

b) How many hours would that student in part (a) need to study to have a 50% chance of getting an A in the class?

Now, we don't know how many studying hour is, we need to find out $X_1$. 
$$
\frac{e^{-6 + 0.05X_1 + 1*3.5}}{(1 + e^{-6 + 0.05X_1 + 1*3.5})} = 0.5
$$
And we use 1- 100 to test which hour ($X_1$) is exactly matched to 50% chance of getting an A. The answer is 50 hours.
```{r, warning=FALSE}
hours <- seq(1 ,100 ,1)
probs <- mapply(hours, 3.5, FUN = prob) #X1, X2, function

names(probs) <- paste(hours,"Hours")
probs %>% 
  tidy() %>%
  filter(x == 0.5)
```

# Exercise 2 (10 points)

Suppose that we take a data set, divide it into equally-sized training and test sets, and then try out two different classification procedures. First, we use logistic regression and get an error rate of 20% on the training data and 30% on the test data. Next, we use 1-nearest neighbors (i.e. $K = 1$) and get an average error rate (averaged over both test and training data sets) of 18%. Based on these results, which method should we prefer to use for classification of new observations? Why?

# Exercise 3 (15 points)
You will in this exercise examine the differences between LDA and QDA.

a) If the Bayes decision boundary is linear, do we expect LDA or QDA to perform better on the training set? On the test set?


b) If the Bayes decision boundary is non-linear, do we expect LDA or QDA to perform better on the training set? On the test set?

c) In general, as the sample size n increases, do we expect the test prediction accuracy or QDA relative to LDA to improve, decline, or be unchanged? Why?

d) True or False: Even if the Bayes decision boundary for a given problem is linear, we will probably achieve a superior test error rate using QDA rather than LDA because QDA is flexible enough to model a linear decision boundary. Justify your answer.

Ans: False. 

# Exercise 4 (20 points)

In this exercise, we will explore a data set about cars called `auto` which you can find [here](data/auto.csv).

The data set contains 1 factor variable and 6 numeric variables. The factor variable `mpg` has two levels `high` and `low` indicating **whether the car has a high or low miles per gallon**. We will in this exercise investigate if we are able to use a logistic regression classifier to predict if a car has high or low mpg from the other variables.

a) Read in the data and create a test-train `rsplit` object of `auto` using `initial_split()`. Use default arguments for `initial_split()`.
```{r}
# read the data
autoDF <- read_csv("data/auto.csv")
# see each variable's type
str(autoDF)
# mutate the response variable to be a factor type
autoDF %>%
  mutate(mpg = as.factor(mpg)) -> autoDF
# set a seed
set.seed(1)
# splitting to train and test sets
auto_split <- initial_split(autoDF, strata = mpg)
auto_split
```

b) Create the training and testing data set with `training()` and `testing()` respectively.
```{r}
auto_train <- training(auto_split)
auto_train %>%
  nrow()
auto_test <- testing(auto_split)
auto_test %>%
  nrow()
```

c) Fit a logistic regression model using `logistic_reg()`. Use all the 6 numeric variables as predictors (a formula shorthand is to write `mpg ~ .` where `.` means everything. Remember to fit the model only using the training data set.
```{r}
# create a model formula
auto_formula <- mpg ~ .

# set the logistic regression engine
lr_spec <- logistic_reg() %>%
  set_engine("glm") %>%
  set_mode("classification")

# fit the model
lr_spec %>%
  fit(auto_formula, data = auto_train) -> lr_fit
lr_fit
```
d) Inspect the model with `summary()` and `tidy()`. Which of the variables are significant?

According to the outcomes of two tables below, we find `intercept`, `weight` and `year` predictors are significant. That is, $b_0$, $b_4$, and $b_6$.
```{r}
# using summary()
lr_fit %>%
  pluck("fit") %>% # unest the list
  summary()

# using tidy()
tidy(lr_fit) %>%
  mutate(signPredictor = ifelse(p.value < 0.05, "Yes", "No")) %>%
  filter(signPredictor == "Yes")
```

e) Predict values for the training data set and save them as `training_pred`.

In order to successfully combine two datasets without has conflicted columns in the following question, we will use `predict()` rather than `augment()`.
```{r}
predict(lr_fit, new_data = auto_train) -> training_pred
```

f) Use the following code to calculate the training accuracy (`auto_training` should be renamed to match your training data set if needed).

The training accuracy in the training data is 90.3 %, which is excellent.
```{r}
bind_cols(training_pred, auto_train) %>%
accuracy(truth = mpg, estimate = .pred_class)
```
g) Predict values for the testing data set and use the above code to calculate the testing accuracy. Compare.

The training accuracy in the testing data is 89.9 %, which is outstanding.
```{r}
augment(lr_fit, new_data = auto_test) -> testing_pred 
accuracy(testing_pred, truth = mpg, estimate = .pred_class)
```
When compared to the training accuracy in both the training and testing datasets, the result is extremely close. Regardless, the high values of predicted accuracy have already indicated that the predictors are good in the `auto` data frame to predict whether the car has a high or low miles per gallon tendency (`mpg`). 

# Execise 5 (20 points)

This exercise should be answered using the `Weekly` data set, which is part of the `LSLR` package. If you don't have it installed already you can install it with

```{r}
# if necessary please run the below line
# install.packages("ISLR")
```

To load the data set run the following code
```{r}
library(ISLR)
data("Weekly")
```

This data is similar in nature to the `Smarket` data from chapter 4's lab, it contains 1089 weekly returns for 21 years, from the beginning of 1990 to the end of 2010.
```{r}
Weekly %>%
  head()
```
a. Produce some numerical and graphical summaries of the data. Does there appear to be any patterns?

Although we can see the approximate range value for each variable, the summary table does not reveal any significant relationship between these variables.
```{r}
# numerical summary
summary(Weekly)
```

The table of correlations is shown below. Suggest that we plot this table so that it is clear to the reader.
```{r}
cor(Weekly[-9]) %>%
  as_tibble()
```

Removing `Direction` variable as it is a qualitative variable. We can see the correlation between these eight variables. The plot shows `Year` and `Volume` has a strong linear relationship.
```{r}
# graphical summary
corrplot(cor(Weekly[1:8]), method="square")
```

This plot depicts an increasing volume of stock trading over time; the volume continues to rise as time passes. Is everyone now wealthy?
```{r}
Weekly %>%
  ggplot(aes(as.factor(Year), Volume)) +
  geom_boxplot() +
  theme_bw()
```

b. Use the whole data set to perform a logistic regression (with `logistic_reg()`) with `Direction` as the response and the five lag variables plus `Volume` as predictors. Use the `summary()` (remember to do `summary(model_fit$fit)`) function to print the results. Do any of the predictors appear to be statistically significant? if so, which ones?

The p-value of intercept and $b_2$ are statistically significant at the level of significance $\alpha =0.05$. That is, the predictor `Lag2` can be rejected the null hypothesis, meaning that there is evidence to say `Lag2` has an impact on `Direction` while the other predictors hold constant.
```{r}
lr_spec %>%
  fit(Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume, data = Weekly) -> lr_Weekly_fit

lr_Weekly_fit$fit %>%
  summary()
```

c. Use `conf_int()` and `accuracy()` from `yardstick` package to calculate the confusion matrix and the accuracy (overall fraction of correct predictions). Explain what the confusion matrix is telling you about the types of mistakes made by logistic regression.

As shown in the confusion matrix, the logistic regression model operate the five `Lag` variables and `Volume` as predictors. The predictions are represented by the rows of the confusion matrix, while the ground truth is represented by the columns. The up-left area represents true **Down**, and the down-right area represents true **Up**; these two values are a leading indicator for determining whether the model is right or wrong. The true Down is 54, which greater then the false Down 48, and the true UP is 557, which is greater than false negative 430. Compared to the ground truth, the predictions don't work well. The model just splitting approximately 50 % to the down and 50 % to the up direction.
```{r}
preds_Weekly <- augment(lr_Weekly_fit, new_data = Weekly)
preds_Weekly %>%
  conf_mat(truth = Direction, estimate = .pred_class) -> confusionMatrixWeekly
confusionMatrixWeekly

# we can also plot the confusion matrix
confusionMatrixWeekly %>%
  autoplot(type = "heatmap") +
  theme_minimal()

```
Look at the table below, which indicates the accuracy of the data that are predicted correctly. The model only predict the direction correctly 611 (54+557) weeks out of 1089 weeks, for an accuracy of 0.56.
```{r}
# accuracy
preds_Weekly %>%
accuracy(truth = Direction, estimate = .pred_class)
```

d. Split the data into a training and testing data set using the following code
```{r}
weekly_training <- Weekly %>% filter(Year <= 2008)
weekly_testing <- Weekly %>% filter(Year > 2008)
```

e. Now fit the **logistic regression model** using the training data, with `Lag2` as the only predictor. Compute the confusion matrix and accuracy metric using the testing data set.

The intercept and `Lag2` are in the significant level.
```{r}
lr_spec %>%
  fit(Direction ~ Lag2, data = weekly_training) -> lr_Lag2_fit

lr_Lag2_fit$fit %>%
  tidy() %>%
  mutate(hypo = ifelse(p.value < 0.05, "H1", "H0"))
```

Only Keeping `Lag2` as predictor, the predicted accuracy of the logistic regression model augments to 0.625.
```{r}
preds_lr_Lag2_fit <- augment(lr_Lag2_fit, new_data = weekly_testing)

# confusion matrix table
preds_lr_Lag2_fit %>%
  conf_mat(truth = Direction, estimate = .pred_class) -> lrMatrix
lrMatrix

# plot the confusion matrix
lrMatrix %>%
  autoplot(type = "heatmap") +
  theme_tufte() +
  ggtitle("Confusion Matrix")

# accuracy
preds_lr_Lag2_fit %>%
accuracy(truth = Direction, estimate = .pred_class) %>%
  mutate(model = "Logistic Regression") -> lr_acc
lr_acc 
```

f. Repeat (e) using LDA.
```{r}
# set LDA specification
lda_spec <- discrim_linear() %>%
  set_mode("classification") %>%
  set_engine("MASS") # the default is "MASS", otherwise is "mda"
```

Fit the linear discriminant analysis model. 
```{r}
lda_Lag2_fit <- fit(lda_spec, Direction ~ Lag2, data = weekly_training)
lda_Lag2_fit
```
The result is very similar to the logistic regression model we mentioned before. Using Linear Discriminant Analysis, the model predicts the `Direction` trend with 0.625 accuracy.
```{r}
preds_lda_Lag2_fit <- augment(lda_Lag2_fit , new_data = weekly_testing)

# confusion matrix table
preds_lda_Lag2_fit %>%
  conf_mat(truth = Direction, estimate = .pred_class) -> ldaMatrix
ldaMatrix

# plot the confusion matrix
ldaMatrix %>%
  autoplot(type = "heatmap") +
  theme_tufte() +
  ggtitle("Confusion Matrix")

# accuracy
preds_lda_Lag2_fit %>%
accuracy(truth = Direction, estimate = .pred_class) %>%
  mutate(model = "Linear Discriminant Analysis") -> lda_acc
lda_acc
```

g. Repeat (e) using QDA.
```{r}
# set QDA specification
qda_spec <- discrim_regularized() %>%
  set_mode("classification") %>%
  set_args(frac_common_cov = 0, frac_identity = 0) %>%
  set_engine("klaR")
```

Fit the Quadratic Discriminant Analysis model. 
```{r}
qda_Lag2_fit <- fit(qda_spec, Direction ~ Lag2, data = weekly_training)
qda_Lag2_fit
```

The Quadratic Discriminant Analysis model has a lower accuracy of 0.585 than the LDA and logistic regression models. Look at the confusion matrix, the model cannot predict anything in downward weekly trends, causing a main weakness to decrease the predicted accuracy of the model.
```{r}
preds_qda_Lag2_fit <- augment(qda_Lag2_fit , new_data = weekly_testing)

# confusion matrix table
preds_qda_Lag2_fit %>%
  conf_mat(truth = Direction, estimate = .pred_class) -> qdaMatrix
qdaMatrix

# plot the confusion matrix
qdaMatrix %>%
  autoplot(type = "heatmap") +
  theme_tufte() +
  ggtitle("Confusion Matrix")

# accuracy
preds_qda_Lag2_fit %>%
accuracy(truth = Direction, estimate = .pred_class) %>%
  mutate(model = "Quadratic Discriminant Analysis") -> qda_acc
qda_acc
```

h. Repeat (e) using KNN with `K = 1`.
```{r}
# set a KNN specification
knn_spec <- nearest_neighbor(neighbors = 1) %>%
  set_mode("classification") %>%
  set_engine("kknn")
```

Fit the K-Nearest Neighbors algorithm model. 
```{r}
knn_Lag2_fit <- fit(knn_spec, Direction ~ Lag2, data = weekly_training)
knn_Lag2_fit
```
The K-Nearest Neighbors algorithm produced a classification model with a 50% accuracy rate, which is the same as random chance.
```{r}
preds_knn_Lag2_fit <- augment(knn_Lag2_fit , new_data = weekly_testing)

# confusion matrix table
preds_knn_Lag2_fit %>%
  conf_mat(truth = Direction, estimate = .pred_class) -> knnMatrix
knnMatrix

# plot the confusion matrix
knnMatrix %>%
  autoplot(type = "heatmap") +
  theme_tufte() +
  ggtitle("Confusion Matrix")

# accuracy
preds_knn_Lag2_fit %>%
accuracy(truth = Direction, estimate = .pred_class) %>%
  mutate(model = "K-Nearest Neighbors") -> knn_acc
knn_acc
```
 
i. Which of these methods appear to provide the best results on the data?

Using `Lag2` as predictor only to predict upward and downward in`Direction` in `Weekly` dataset, the **Logistic Regression** and the **Linear Discriminant Analysis** models are the best methods based on the accuracy rate with descending order.
```{r}
lr_acc %>%
  bind_rows(lda_acc, qda_acc, knn_acc) %>%
  arrange(desc(.estimate))
```
Alternatively, we may use another method that requires great statistical senses and programming skills to compare several models.
```{r}
# crate a model list we yielded before
allModels <- list("Logistic Regression" = lr_Lag2_fit,
                  "LDA" = lda_Lag2_fit,
                  "QDA" = qda_Lag2_fit,
                  "KNN" = knn_Lag2_fit)
```
Then, using the `purrr` package's `imap dfr()`, apply `augment()` to each of the models using the testing data set. `.id = "model` adds a column called "model" to the resulting tibble that contains the names of `allModels`.
```{r}
allPreds <- imap_dfr(allModels, augment, 
                  new_data = weekly_testing, .id = "model")

allPreds %>%
  select(model, Direction, .pred_class, .pred_Down, .pred_Up) %>%
  head()
```
`yardstick()` package provides multiple different metrics by using `metric_set()`. 
```{r}
multi_metric <- metric_set(accuracy, sensitivity, specificity)
```
- Using `group_by()` and calculate the metrics for each model. 
- Using `desc()` to see the model performances with different matrices.
```{r}
allPreds %>%
  group_by(model) %>%
  multi_metric(truth = Direction, estimate = .pred_class) %>%
  arrange(desc(.estimate))
```
`yardstick()` package also gives a function to create receiver operating characteristic (ROC) curves by using `roc_curve()`. Because the LDA curve is perfectly hidden behind the logistic regression, it is not visible here.
```{r}
allPreds %>%
  group_by(model) %>%
  roc_curve(Direction, .pred_Down) %>%
  autoplot() +
  ggtitle("ROC Curves")
```

j. (Optional) Experiment with different combinations of predictors for each of the methods. Report the variables, methods, and associated confusion matrix that appears to provide the best results on the held-out data. Note that you can also experiment with different values of K in KNN. (This kind of running many many models and testing on the testing data set many times is not good practice. We will look at ways in later weeks on how we can properly explore multiple models).

There are many methods to select the predictors. In this session, we only focus on the value of neighbors in KNN model. To find the best value of k, take the square root of the total number of observations (1089), which is 33. We will start with 33 neighbors and then look for the best value of k.
```{r}
sqrt(nrow(Weekly))
```

```{r}
# set a KNN specification
knn_spec_33 <- nearest_neighbor(neighbors = 33) %>%
  set_mode("classification") %>%
  set_engine("kknn")
```

Fit the K-Nearest Neighbors algorithm model. 
```{r}
knn33_Lag2_fit <- fit(knn_spec_33, Direction ~ Lag2, data = weekly_training)
knn33_Lag2_fit
```
The predicted accuracy of the `knn33_Lag2_fit` model is 6% higher than that of the previous KNN model (K=1).
```{r}
preds_knn33_Lag2_fit <- augment(knn33_Lag2_fit , new_data = weekly_testing)

# confusion matrix table
preds_knn33_Lag2_fit %>%
  conf_mat(truth = Direction, estimate = .pred_class) -> knn33Matrix
knn33Matrix

# plot the confusion matrix
knn33Matrix %>%
  autoplot(type = "heatmap") +
  theme_tufte() +
  ggtitle("Confusion Matrix")

# accuracy
preds_knn33_Lag2_fit %>%
accuracy(truth = Direction, estimate = .pred_class) %>%
  mutate(model = "KNN with 33 Neighbors") -> knn33_acc
bind_rows(knn33_acc, knn_acc) %>%
  mutate(model = recode(model, "K-Nearest Neighbors" = "KNN with 1 Neighbor"))
```

# References
- http://uc-r.github.io/discriminant_analysis
- https://emilhvitfeldt.github.io/ISLR-tidymodels-labs/classification.html
- https://rstudio-pubs-static.s3.amazonaws.com/316172_a857ca788d1441f8be1bcd1e31f0e875.html
