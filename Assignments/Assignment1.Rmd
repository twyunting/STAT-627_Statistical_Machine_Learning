---
title: "STAT-627 Assignment 1"
author: "Yunting Chiu"
date: "`r Sys.Date()`"
output:
  html_document: 
    number_sections: yes
    highlight: pygments
    theme: cerulean
urlcolor: blue
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
# Exercise 1
For each of parts (a) through (d), indicate whether we would generally expect the performance of a flexible statistical learning method to be better or worse than an inflexible method. Justify your answer.

Before we answering the questions, we should know a inflexible method is a simple method; a flexible method is a complex method.

The sample size n is extremely large, and the number of predictors p is small.

- **Better**. The large number of observations is better because we can know more about the detailed correlation between them. Also, the number of small predictors and large sample sizes may help to avoid a overfitting problem and reduce the bias.

The number of predictors p is extremely large, and the number of observations n is small.

- **Worse**. It is possible that not all predictors have a significant effect on the response variable. Overfitting and spurious correlation are phenomenons that affect data models with a large number of predictors, in which the data model performs well on training data but badly on test data. Furthermore, small sample sizes and too many predictors will result in high variance.

The relationship between the predictors and response is highly non-linear.

- **Better**. If there are too many limits, just a few types of trends are conceivable, which may not convey the underlying nonlinear relationship. A better fit will result from having more degrees of freedom. Therefore, non-linearity allows predictors to be better fitted with the dependent variable.

The variance of the error terms, is extremely high.

- **Worse**. Because of the high variance, an algorithm may model the random noise training data rather than the expected outputs. High error terms would decrease the model's performance.

## References:
1. https://www.kdnuggets.com/2017/04/must-know-fewer-predictors-machine-learning-models.html
2. https://stats.stackexchange.com/questions/69237/flexible-and-inflexible-models-in-machine-learning
3. https://becominghuman.ai/machine-learning-bias-vs-variance-641f924e6c57

# Exercise 2
Describe the difference between a parametric and non-parametric statistical learning approach. What are the advantages of a parametric approach to regression or classification (as opposed to a non-parametric approach)? What are its disadvantages?

Parametric statistical approaches rely on assumptions about the underlying population's distribution shape (i.e., a normal distribution) and the assumed distribution's form or parameters (i.e., means and standard deviations). Nonparametric statistical approaches make few or no assumptions on the form of the population distribution from which the sample was taken. In other words, the distinction between parametric and non-parametric statistical learning approaches is based on assumptions. 

a. Advantages of parametric statistical learning: Easy to figure out. If the each group is distinct, the parametric method can be highly effective.
b. Disadvantages of parametric statistical learning: If the data does not meet the assumptions, the parametric method could lead to incorrect conclusions. The assumption part is very important in parametric approaches. If the data is unable to overcome the assumption obstacle, consider transforming the data as a remedy. Also, small sample size (n < 30) is not good for parametric methods.
c. Advantages of non-parametric statistical learning: A good choice for data with a small sample size. There is no need to pass any assumption tasks ahead of time.
d. Disadvantages of non-parametric statistical learning: If the data is surely normal, non-parametric have less power for the sample size than parametric methods. Plus, non-parametric processes can likewise be more difficult to interpret than parametric processes.

## References
1. https://www.mayo.edu/research/documents/parametric-and-nonparametric-demystifying-the-terms/doc-20408960
2. https://www.thoughtco.com/parametric-and-nonparametric-methods-3126411

# Exercise 3
Carefully explain the the difference between the KNN classifier and KNN regression methods. Name a downside when using this model on very large data.

In KNN classification algorithm, the user wants to compute a categorical value, which is represented by an integer. For example, similarly to a dummy variable, we set 0 as male and 1 as female. The KNN classification algorithm will examine the k closest neighbors of the input we are attempting to predict. The most common result among the k samples will then be output. 

In KNN algorithm for regression, the user wishes to output a numerical value, such as rent prices or cryptocurrency price predictions. The KNN algorithm would combine the values associated with the k nearest examples from the one on which we wish to make a prediction into a single value by taking an average or median to be a result.

When using a large data set, the prediction stage might be slow. It also requires high memory because we need to store all of the training data. Given that, it can also be computationally expensive. 

## References
1. https://www.mygreatlearning.com/blog/knn-algorithm-introduction/
2. https://stackoverflow.com/questions/64990030/difference-between-classification-and-regression-in-k-nearest-neighbor
3. https://cs231n.github.io/classification/

# Exercise 4
Suppose we have a data set with five predictors, $X1$= GPA, $X2$= extracurricular activities (EA), $X3$= Gender (1 for Female and 0 for Male), $X4$= Interaction between GPA and EA, and $X5$= Interaction between GPA and Gender. The response is starting salary after graduation (in thousands of dollars). Suppose we use least squares to fit the model, and get $\beta_0$ = 50, $\beta_1$ = 20, $\beta_2$ = 0.07, $\beta_3$ = 35, $\beta_4$ = 0.01, $\beta_5$ = âˆ’10.

The regression equation should be:
$$
\hat{salary} = 50 + 20GPA + 0.07EA + 35Gender + 0.01GPA*EA -10GPA*Gender
$$

As `Gender` is a dummy variable, the equation for females and males should be:

**Female**: gender is 1
$$
\hat{salary} = 50 + 20GPA + 0.07EA + 35 + 0.01GPA*EA -10GPA*Gender
$$
$$
\hat{salary} = 85 + 10GPA + 0.07EA + 0.01GPA*EA
$$

**Male**: gender is 0
$$
\hat{salary} = 50 + 20GPA + 0.07EA + 0.01GPA*EA 
$$
In other words, the difference between female and male is:
$$
female - male = (85 + 10GPA + 0.07EA + 0.01GPA*EA) - (50 + 20GPA + 0.07EA + 0.01GPA*EA)
$$
$$
female - male = 35 - 10GPA
$$
a. Which answer is correct, and why?\
    1. For a fixed value of EA and GPA, males earn more on average than females.\
    2. For a fixed value of EA and GPA, females earn more on average than males.\
    3. For a fixed value of EA and GPA, males earn more on average than females provided that the GPA is high enough.\
    4. For a fixed value of EA and GPA, females earn more on average than males provided that the GPA is high enough.

- (3) is correct. Because the expected salary of female subtract male is 35 - 10 * GPA, we can know the female earn more **35 - 10 * mean(GPA)** than male. According to the **female-male equation**, as the average GPA rises, the female will earn less. In conclusion, men will earn more than women if their GPA is higher.

b. Predict the salary of a female with EA of 110 and a GPA of 4.0.

- 137,100 dollars.
```{r}
EA <- 110
GPA <- 4
f_salary <- (85 + 10*GPA + 0.07*EA + 0.01*GPA*EA) * 1000 # in thousand dollars
f_salary 
```


c. True or false: Since the coefficient for the GPA/EA interaction term is very small, there is very
little evidence of an interaction effect. Justify your answer.

- This is **false**. A small value of coefficient does not imply that the interaction term has a minor influence. The statistical significance of the coefficient can be determined by looking at the p-value in the coefficient table.

# Exercise 5
This question should be answered using the `biomass` data set.
```{r}
library(tidyverse)
library(tidymodels)
data("biomass")
biomass %>%
  head()
```
a. Fit a multiple regression model to predict `HHV` using `carbon`, `hydrogen` and `oxygen`.
```{r}
# create a parsnip specification
linear_reg() %>%
  set_mode("regression") %>%
  set_engine("lm") -> lm_spec

# fit the model using tidymodel
lm_spec %>%
  fit(HHV ~ carbon + hydrogen + oxygen, data = biomass) -> lm_fit

# Another way to see the summary table
# tidy(lm_fit)

# predict HHV variable by using the `lm_fit` linear model
predict(lm_fit, new_data = biomass) 
```

b. Provide an interpretation of each coefficient in the model. 

- The p-value of intercept is 0.0952, meaning that we fail to reject the null.
- The p-value of $b_1$ is less than 2e-16, meaning that the `carbon` has an impact on `HHV` when holding `hydrogen` and `oxygen` constant.
- The p-value of $b_2$ is 0.0000986, meaning that the `hydrogen` has an impact on `HHV` when holding `carbon` and `oxygen` constant.
- The p-value of $b_3$ is 0.9638, which means the result is the same as the $b_0$. That is, we have no evidence to reject the null hypothesis. So we can conclude the `oxygen` has no impact on `HHV` when holding `carbon` and `hydrogen` constant.
- $b_0$ is 1.0456860, $b_1$ is 0.3478508, $b_2$ is 0.2430900, and $b_3$ is -0.0003767.
```{r}
lm_fit %>%
  pluck("fit") %>%
  summary()
```

c. Write out the model in equation form.
$$
\hat{HHV} = 1.0456860 + 0.3478508carbon + 0.2430900hydrogen - 0.0003767oxygen
$$

d. For which the predictors can you reject the null hypothesis H0:$\beta_j = 0$?

Hypothesis test: H0: $\beta j=0$ vs Ha: $\beta j \neq{0}$

The p-value of $b_1$ and $b_2$ is 2e-16 and 0.0000986, so we have evidence to reject the null hypothesis in favor of the alternative hypothesis, meaning that $b_1$ and $b_2$ are not equal to 0. Thus, predictor `carbon` and `hydrogen` have an impact on `HHV`, when holding `oxygen` constant.

e. On the basis of your response to the previous question, fit a smaller model that only uses the predictors for which there is evidence of association with the outcome.

We consider removing the predictor that does not meet the significance level based on the results of 5a. That is, we should eliminate the variable `oxygen` (because of no impact) then re-fit the linear model. 
```{r}
lm_spec %>%
  fit(HHV ~ carbon + hydrogen, data = biomass) -> lm_new_fit

lm_new_fit$fit %>%
  summary()
```

f. How well do the models in (a) and (e) fit the data? How big was the effect of removing the predictor?

The intercept and `oxygen` are not significant in the **5a** model, but all of the predictors and the intercept are significant in the **5e** model. Plus, the adjusted R-squared is 0.8509 in 5a, which is less than the adjusted R-squared in 5e (0.8512). As a result, removing the predictor `oxygen` is a wise decision.


