---
title: "STAT-627 Assignment 5"
author: "Yunting Chiu"
date: "`r Sys.Date()`"
output:
  html_document: 
    number_sections: yes
    theme: cerulean
    highlight: kate
urlcolor: blue
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(tidymodels)
library(ggthemes)
```

In this assignment, I will be using [Tidymodels](https://www.tidymodels.org/) framework instead of base R.

# Exercise 1 (10 points)

Suppose we fit a curve with basis functions $b_1(X) = X$, $B_2(X) = (X - 1)^2 I(X \geq 1)$. Note that $I(X \geq 1)$ equals 1 for $X \geq 1$ and 0 otherwise. We fit the linear regression model

$$Y = \beta_0 + \beta_1b_1(X) + \beta_2b_2(X) + \varepsilon$$

and obtain the coefficient estimates $\hat \beta_0 = 1$, $\hat \beta_1 = 1$, $\hat \beta_2 = -2$. Sketch the estimated curve between $X = -2$ and $X = 2$. Note the intercepts, slopes and other relevant information.
 
- If $X < 1$, the estimated curve is $\hat{f}(X) = 1+ X$ . The intercept and slope are both 1.
- If $X>= 1$, the estimated curve: $\hat{f}(X) = 1 + X-2(X - 1)^2$.
```{r}
X <- seq(-2, 2, 0.01)
Y <-  1 + X + -2 * (X - 1)^2 * (X >= 1)
df1 <- tibble(X, Y)

ggplot(df1, aes(X, Y)) + 
  geom_vline(xintercept = 0) + 
  geom_hline(yintercept = 0) + 
  geom_vline(xintercept = 1, col = "red") + 
  geom_line(size = 3, color = "green") +
  theme_bw()
```

# Exercise 2 (10 points)

Suppose we fit a curve with basis functions
$b_1(X) = I(0 \leq X \leq 2) - (X-1)I(1 \leq X \leq 2)$, 
$B_2(X) = (X - 3) I(3 \leq X \leq 4) + I(4 < X \leq 5)$. We fit the linear regression model

$$Y = \beta_0 + \beta_1b_1(X) + \beta_2b_2(X) + \varepsilon$$
and obtain the coefficient estimates $\hat \beta_0 = 1$, $\hat \beta_1 = 1$, $\hat \beta_2 = 3$. Sketch the estimated curve between $X = -2$ and $X = 2$. Note the intercepts, slopes and other relevant information.

-  If $X < 0$, the $\hat{f}(X) = 1$. When $X < 0$, we can see the horizontal line, indicating that the slope is zero.
- If $0 <= X < 1$, the $\hat{f}(X) = 1 + 1 = 2 $. The horizontal line between 0 to 1 still indicating that the slope is zero.
- If $0 <= X < 1$, the $\hat{f}(X) = 1 + 1 - (X - 1) * 1 = - X +3 $. Now, the slope is -1.
```{r}
X <- seq(-2, 2, 0.01)
Y <- 1 + (X >= 0 & X <= 2) - (X - 1)*(X >= 1 & X <= 2) + 
     3*(X - 3)*(X >= 3 & X <= 4) + 3*(X > 4 & X <= 5)

df2 <- tibble(X, Y)

ggplot(df2, aes(X, Y)) + 
  geom_vline(xintercept = 0) + 
  geom_hline(yintercept = 0) + 
  geom_line(size = 3, color = "blue") +
  theme_bw()
```

# Exercise 3 (10 points)

Explain what happens to the bias/variance trade-off of our model estimates use regression splines.


# Exercise 4 (10 points)

Draw an example (of your own invention) of a partition of two-dimensional feature space that could result from recursive binary splitting. Your example should contain at least six regions. Draw a decision tree corresponding to this partition. Be sure to label all aspects of your figures, including regions $R_1, R_2, ...$, the cut points $t_1, t_2, ...$, and so forth.

# Exercise 5 (10 points)

Provide a detailed explanation of the algorithm that is used to fit a regression tree.

# Exercise 6 (10 points)

Explain the difference between bagging, boosting, and random forests.

# Exercise 7 (20 points)

You will be using the Boston data found [here](data/Boston.csv). The response is `medv` and the remaining variables are predictors.

Do test-training split as usual, and fit a random forest model or boosted tree (your choice) and a linear regression model.

The random forest or boosted tree model has a selection of hyper-parameters that you can tune to improve performance. Perform hyperparameter tuning using k-fold cross-validation to find a model with good predictive power. How does this model compare to the linear regression model?