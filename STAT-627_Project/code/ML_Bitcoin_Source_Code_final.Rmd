---
title: "Statistical Machine Learning for Bitcoin Prediction"
author: "Yunting Chiu, Haiman Wong"
date: "`r Sys.Date()`"
output:
  html_document: 
    toc: yes
    toc_depth: 2
    theme: cerulean
  pdf_document:
    toc: yes
    toc_depth: 2
    fig_caption: yes
subtitle: 'Statistical Machine Learning Project'
urlcolor: blue
linkcolor: red
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# 1. Abstract
As Wall Street giants, retail investors, and aspiring cryptocurrency trailblazers continue to flood the cryptocurrency market, the ability to predict the volatility of cryptocurrency stocks has proven to be increasingly invaluable. In this report, we detail our methodology that applies statistical machine learning techniques to predict the direction of Bitcoin stocks. Our work also aims to build upon previous research conducted in anticipating trends within cryptocurrency using statistical machine learning methods. To predict whether the direction of Bitcoin stocks will increase or decrease on a given date, we employ Linear Discriminant Analysis (LDA), Quadratic Discriminant Analysis (QDA), K-Nearest Neighbors (KNN), Logistic Regression Analysis, Random Forest, and Decision Trees. We also perform statistical analyses of our dataset using simple linear regression, multiple linear regression, and summary statistics, and visualize these metrics to gain a more comprehensive understanding of the variables that may contribute to deciding the direction of Bitcoin stocks on a given day. Finally, we analyze our results and discuss opportunities for future work and research. 

# 2. Install the Libraries
```{r, message=FALSE}
library(tidyverse)
library(tidymodels)
library(discrim)
library(corrplot)
library(rpart.plot)
library(vip)
library(ranger)
```

# 3. BitCoin Data 
## Data Tidying and Cleaning

The data spans the years 2013-10-01 to 2021-06-13.
```{r}
#setwd("/Users/haimanwong/Desktop")
BTC <- read_csv("./data/BTC_USD_2013-10-01_2021-06-13-CoinDesk.csv")

BTC %>%
  rename(Closing_Price = "Closing Price (USD)",
         Open_Price = "24h Open (USD)") %>%
  mutate(Lag1 = (lag(Closing_Price, n = 1) - lag(Open_Price, n = 1)) / lag(Open_Price, n = 1)*100,
         Lag2 = (lag(Closing_Price, n = 2) - lag(Open_Price, n = 2)) / lag(Open_Price, n = 2)*100,
         Lag3 = (lag(Closing_Price, n = 3) - lag(Open_Price, n = 3)) / lag(Open_Price, n = 3)*100,
         Lag4 = (lag(Closing_Price, n = 4) - lag(Open_Price, n = 4)) / lag(Open_Price, n = 4)*100,
         Lag5 = (lag(Closing_Price, n = 5) - lag(Open_Price, n = 5)) / lag(Open_Price, n = 5)*100,
         Return_Today = (Closing_Price - Open_Price) / Open_Price*100,
         Direction_Today = ifelse(Return_Today > 0, "Up", "Down"),
         Direction_Today = as.factor(Direction_Today)
         ) -> BTC_tidied 

names(BTC_tidied)
head(BTC_tidied)
tail(BTC_tidied)
```

## Data Dictionary

|Variable          |Class     |Description |
|:-----------------|:---------|:-----------|
|Currency          |character    | Bitcoin or BTC|
|Date              |date      | From  2013-10-01 to 2021-06-13|
|Closing_Price     |double    | The price at the end of a trading day (24hr)|
|Open_Price        |double    | The price at the beginning of a trading day (24hr)|
|24h High (USD)    |double    | The day's highest price |
|24h Low (USD)     |double    | The day's lowest price |
|Lag1              |double    | Percentage return for previous day |
|Lag2              |double    | Percentage return for 2 days previous |
|Lag3              |double    | Percentage return for 3 days previous |
|Lag4              |double    | Percentage return for 4 days previous |
|Lag5              |double    | Percentage return for 5 days previous |
|Return_Today      |double    | Percentage return for today |
|Return_Direction  |factor    | A factor with levels Down and Up indicating whether the market had a positive or negative return on a given day |

# 4. Data Analysis
## Exploratory Data Analysis

Drop NAs. Because we mutated `Lag1` to `Lag5`, resulting in 5 observations that include NAs.
```{r}
BTC_tidied %>%
  drop_na() -> BTC_tidied
```

We can see that the vast majority of variables are numerical.
```{r}
str(BTC_tidied)
```

There are 2813 observations and 13 variables in the data set now.
```{r}
dim(BTC_tidied)
```


```{r}
summary(BTC_tidied)
```

To understand the relationship between multiple variables in the dataset so we removed the non-numerical variables.
```{r}
correlation <- cor(BTC_tidied[, c(-1, -2, -13)])
correlation
```

# 5. Data Visualization

## Correlation Table
When variables are visualized, it is easier to read the correlation between them. The `Closing_Price`, `Open_Price`, `24 High (USD)` and `24 Low (USD)` have a high correlation. Because `Direction_Today` is a response variable, if `Return_Today` is a predictor, which makes no sense. Thus, we just keep `Lag1`, `Lag2`, `Lag3`, `Lag4`, and `Lag5` as predictors in the initial stage.
```{r}
corrplot(correlation, method = "color", addCoef.col = "black", 
         number.cex = 0.7)
```

## Bitcoin to the moon? 
The scatterplot created using the ggplot function allows us to review the relationship between the Closing_Price variable and the Date variable. 
```{r}
BTC_tidied %>%
  ggplot(aes(Date, Closing_Price)) +
  geom_point(color = "blue") +
  theme_bw()
```

## Bitcoin to the moon (with Opening Price)
Similar to the scatterplot created above, using the ggplot function allows us to review the relationship between the Open_Price variable and the Date variable. 
```{r}
BTC_tidied %>%
  ggplot(aes(Date, Open_Price)) +
  geom_point(color = "green") +
  theme_bw()
```

When we compare both scatterplots, we observe that the trends align between the dates. This affirms that when the Open Price on a given day is lower, the Closing Price will match that trend and also be lower. Likewise, when Open Price on a given day is higher, the Closing Price will align with that trend and also be higher. We can also observe that there are three noticeably visible peaks in Opening and Closing Price in 2018, the end of 2019, and mid-2021. 

## Variation of Date vs Return Today
Using the ggplot function again, we can use a bargraph to visualize the Return_Today variable against the Date variable to better understand the relationship between the variables: 
```{r}
ggplot(data = BTC_tidied) +
  geom_bar(mapping = aes(x = Date, y = Return_Today, fill = Date), stat = "identity")
```

The legend indicates that the overall returns day to day had a consistent trend of increases and decreases over the years. While some decreases and increases significantly outweighed others, the overarching trend on this bargraph shows that the frequency of returns increasing and decreasing everyday is variable. 

## Frequency of Ups vs Downs in a Pie Chart
Here, we create a pie chart to map out the frequency that the direction of Bitcoin stocks increases and decreases overall: 
```{r}
cd <- ggplot(BTC_tidied, aes(x="", y = Return_Today, fill = Direction_Today))+
  geom_bar(width = .5, stat = "identity") 
pie <- cd + coord_polar("y", start = 0) 
pie
```

Based on this pie chart, we see that while the pie chart is almost a 50/50 split between increases and decreases, there are still slightly more days where the direction of Bitcoin stocks increases as opposed to decreases. 

# 6. Decision Tree and Random Forest
## Decision Trees

### Decision Tree Model 1 
Here, we create a decision tree model using the default hyperparameters. We begin by setting a seed and creating the initial split, training data set, and testing data set:
```{r}
bc_split <- initial_split(BTC_tidied)
set.seed(1234)
bc_train <- training(bc_split)
bc_test <- testing(bc_split)
```

Now, we create the decision tree model specification by setting the engine equal to rpart and the mode equal to classification:
```{r}
decision_tree_rpart_spec <- 
  decision_tree() %>%
  set_engine('rpart') %>%
  set_mode('classification')
```

Next, we fit the decision tree model against our training data set: 
```{r}
dt_fit <- fit(decision_tree_rpart_spec, Direction_Today ~., data = bc_train)
dt_fit
```

Finally, we plot our decision tree: 
```{r}
rpart.plot(dt_fit$fit)
```

Here, we see that the decision tree has one visible split and that values that are less than -205e-6 are classified as a downward direction, while values that are greater than -205e-6 are classified as an upward direction. We also see that our results align with the frequency that we observed in our pie chart because there are slightly more classifications of Bitcoin stocks increasing at 54% than decreasing at 46%. 

### Decision Tree Model 2: Hyperparameter
Now, we create a second decision tree model variation by tuning the hyperparameter where cost_complexity is equal to 0.5. After tuning the hyperparameter, we  fit the decision tree model against the training data as we did before with the default hyperparameters: 
```{r}
decision_tree_rpart_spec_lambda <-
  decision_tree(cost_complexity = 0.5) %>%
  set_engine('rpart') %>%
  set_mode('classification')
  
dt_fit_lambda2 <- fit(decision_tree_rpart_spec_lambda, Direction_Today ~., data = bc_train)
dt_fit_lambda2
```

Next, we plot our decision tree: 
```{r}
rpart.plot(dt_fit_lambda2$fit)
```

Based on this plot, we see that the decision tree has one visible split and that values that are less than -205e-6 are classified as a downward direction, while values that are greater than -205e-6 are classified as an upward direction. We also see that our results align with the frequency that we observed in our pie chart because there are slightly more classifications of Bitcoin stocks increasing at 54% than decreasing at 46%. This means that tuning the hyperparameter where cost_complexity is equal to 0.5 did not dramatically affect our final decision tree results because the results still match the first decision tree model that we tuned. 

### Decision Tree Model 3
To create a third decision tree model variation, we tune the hyperparameter where cost_complexity is equal to 3. After tuning the hyperparameter, we  fit the decision tree model against the training data as we did before with the default hyperparameters: 
```{r}
decision_tree_rpart_spec_lambda <-
  decision_tree(cost_complexity = 3) %>%
  set_engine('rpart') %>%
  set_mode('classification')
  
dt_fit_lambda3 <- fit(decision_tree_rpart_spec_lambda, Direction_Today ~., data = bc_train)
dt_fit_lambda3
```

Then, we plot our decision tree model with tuned hyperparameters: 
```{r}
rpart.plot(dt_fit_lambda3$fit)
```

Here, it is clear that tuning the hyperparameter of cost_complexity to 3 dramatically affected our decision tree model results. There is barely a visible split here and only the 54% increase is shown. This may also suggest that the tuned hyperparameters of this decision tree model are not that good of a fit for our data since it fails to yield results that meaningfully predict our Bitcoin stock directions. 

### Decision Tree Model 4
To create our fourth decision tree model variation, we tune the cost_complexity hyperparameter to a more moderate value of 1.5 and fit the decision tree model specification to our training data set: 
```{r}
decision_tree_rpart_spec_lambda <-
  decision_tree(cost_complexity = 1.5) %>%
  set_engine('rpart') %>%
  set_mode('classification')
  
dt_fit_lambda4 <- fit(decision_tree_rpart_spec_lambda, Direction_Today ~., data = bc_train)
dt_fit_lambda4
```

Once again, we plot our adjusted decision tree model variation: 
```{r}
rpart.plot(dt_fit_lambda4$fit)
```

Similar to the model where our cost_complexity hyperparameter was tuned to equal 3, we observe that the tuning affected our results quite dramatically. It's interesting that our plot here is the same as the plot generated from our third model where our cost_complexity hyperparameter was tuned to equal 3. Regardless, since there is barely a visible split here and only the 54% increase is shown, this may be an indication that the tuned hyperparameter of this decision tree model are not the best fit for our data since it fails to yield results that meaningfully predict the direction of our Bitcoin stocks on a given day. 

Now that we've run four variations of the decision tree model, we can use the vi() function to find the variable importance for the first decision tree model that we created because it appears to have one of the best fits with the default hyperparameters: 
```{r}
vi(dt_fit)
```

The variable importance indicates that the Return_Today variable ranks the highest in importance. 

Now, we use the vip() function to plot our variable importance for the first decision tree model that we created:
```{r}
vip(dt_fit)
```

The results on the plot created for variable importance align with the numerical values that we generated above using the vi() function. The Return_Today variable is still the most important. 

## Random Forest
Now, we run a random forest model. To create the random forest model specification, we set the engine equal to ranger, importance equal to impurity, and the mode equal to classification: 
```{r}
rand_forest_ranger_spec <- 
  rand_forest() %>%
  set_engine('ranger', importance = "impurity") %>%
  set_mode('classification')
rand_forest_ranger_spec
```

Next, we set the seed and fit the random forest model specification against our training data set. 
```{r}
set.seed(4321)
rf_fit <- fit(rand_forest_ranger_spec, Direction_Today ~., data = bc_train)
rf_fit
```

We can then use the vi() function to find the variable importance. 
```{r}
vi(rf_fit$fit)
```

Here, we see that for the random forest model, the Return_Today variable is the most important. 

Finally, we can plot the variable importance of our random forest model:
```{r}
vip(rf_fit$fit)
```

In doing so, we observe that the Return_Today variable is ranked with the highest importance for the random forest model. 

# 7. Data Modeling

Before classifying `Direction_Today`, we can use linear regression models to examine the entire relationship between `Return_Today` and `Lags1` to `Lags5`. We will concentrate on its coefficients.

## Multiple Linear Regression Model 
Here, we use the Return_Today variable as our response variable and the five Lag variables as our predictors to create our multiple regression model: 
```{r}
btc_mlc <- lm(Return_Today ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5, data = BTC_tidied)
btc_mlc
```

In doing so, we obtain the coefficients for each of the predictor variables and the intercept. We can then use the summary() function to get a better understanding of which predictor variables are significant and how well our model fits our dataset. 
```{r}
summary(btc_mlc)
```

With an adjusted r-squared value of 0.003522 and a multiple r-squared value of 0.005297, our results suggest that the model is not doing the best at explaining the variability in the response variable. Our low adjusted r-squared value also indicates that our model is not telling us very much about how much of the variability for the response variable is due to explanatory variables that have impact on the response variables. However, our p-value of 0.01084 is less than our standard significance level of 0.05, which suggests that we can reject the null hypothesis and our model is significant. We can also observe that only two of our predictor variables, Lag4 and Lag5, are less than our standard significance level of 0.05 and significant. 

We can also use the plot() function to further analyze how well our model performs on our data set:
```{r}
par(mfrow = c(2, 2))
plot(btc_mlc)
```

Overall, all four diagnostic plots suggest that our model is a moderately good fit for our data. There is evidence of some unequal variance and distribution in the Scale-Location and Residuals vs. Leverage plots, but the points still largely follow the fitted line and exhibit some strength in randomization. The QQ plot and Residuals vs. Fitted plots also indicate that our points are fairly normally distributed.  

## Simple Linear Regression Model 1
Now, we evaluate our dataset by running simple linear regression models between each of our predictor variables and our response variable one at a time. Here, we begin with the Lag1 predictor variable and the Return_Today response variable:
```{r}
btc_slr <- lm(Return_Today ~ Lag1, data = BTC_tidied)
btc_slr
```

Upon receiving the coefficients for this first simple linear regression model, we can apply the summary() function to acquire a better evaluation of how well this model fits our data set:

```{r}
summary(btc_slr)
```

With an adjusted r-squared value of 0.0001516 and a multiple r-squared value of 0.0005078, our results suggest that the model is not doing the best at explaining the variability in the response variable. Our low adjusted r-squared value also indicates that our model is not telling us very much about how much of the variability for the response variable is due to explanatory variables that have impact on the response variables. Our p-value of 0.2326 is also greater than our standard significance level of 0.05, which suggests that we fail to reject the null hypothesis and our model is not significant. We can also observe that the predictor variable is not significant here because its p-value is 0.232566, which is greater than our standard significance level of 0.05. 

## Simple Linear Regression Model 2
To create our second simple linear regression model, we use Lag2 as our predictor variable and maintain our response variable as Return_Today: 
```{r}
btc_slr2 <- lm(Return_Today ~ Lag2, data = BTC_tidied)
btc_slr2
```

Upon receiving the coefficients for this first simple linear regression model, we can apply the summary() function to acquire a better evaluation of how well this model fits our data set:

```{r}
summary(btc_slr2)
```

With an adjusted r-squared value of -0.0002747 and a multiple r-squared value of 8.164e-05, our results suggest that the model is not doing the best at explaining the variability in the response variable. Our low adjusted r-squared value also indicates that our model is not telling us very much about how much of the variability for the response variable is due to explanatory variables that have impact on the response variables. Our p-value of 0.6322 is also greater than our standard significance level of 0.05, which suggests that we fail to reject the null hypothesis and our model is not significant. We can also observe that the predictor variable is not significant here because its p-value is 0.6322, which is greater than our standard significance level of 0.05. 

## Simple Linear Regression Model 3
Now, we create our third simple linear regression model by using the Lag3 variable as our predictor and maintain the Return_Today variable as our response: 
```{r}
btc_slr3 <- lm(Return_Today ~ Lag3, data = BTC_tidied)
btc_slr3
```

Upon receiving the coefficients for this first simple linear regression model, we can apply the summary() function to acquire a better evaluation of how well this model fits our data set:

```{r}
summary(btc_slr3)
```

With an adjusted r-squared value of -0.000122 and a multiple r-squared value of 0.0002343, our results suggest that the model is not doing the best at explaining the variability in the response variable. Our low adjusted r-squared value also indicates that our model is not telling us very much about how much of the variability for the response variable is due to explanatory variables that have impact on the response variables. Our p-value of 0.4175 is also greater than our standard significance level of 0.05, which suggests that we fail to reject the null hypothesis and our model is not significant. We can also observe that the predictor variable is not significant here because its p-value is 0.417479, which is greater than our standard significance level of 0.05.

## Simple Linear Regression Model 4
Here, we use the Lag4 variable as our predictor and maintain, once again, that the Return_Today variable is our response:
```{r}
btc_slr4 <- lm(Return_Today ~ Lag4, data = BTC_tidied)
btc_slr4
```

Upon receiving the coefficients for this first simple linear regression model, we can apply the summary() function to acquire a better evaluation of how well this model fits our data set:

```{r}
summary(btc_slr4)
```

With an adjusted r-squared value of 0.001782 and a multiple r-squared value of 0.002138, our results suggest that the model is not doing the best at explaining the variability in the response variable. Our low adjusted r-squared value also indicates that our model is not telling us very much about how much of the variability for the response variable is due to explanatory variables that have impact on the response variables. Our p-value of 0.01428 is less than our standard significance level of 0.05, which suggests that we can reject the null hypothesis and our model is significant. We can also observe that the predictor variable is significant here because its p-value is 0.014277, which is less than our standard significance level of 0.05.

## Simple Linear Regression Model 5
Finally, we can fit our fifth simple linear regression model by using Lag5 as our predictor variable and maintain Return_Today as our response variable: 
```{r}
btc_slr5 <- lm(Return_Today ~ Lag5, data = BTC_tidied)
btc_slr5
```

Upon receiving the coefficients for this first simple linear regression model, we can apply the summary() function to acquire a better evaluation of how well this model fits our data set:

```{r}
summary(btc_slr5)
```

With an adjusted r-squared value of 0.001692 and a multiple r-squared value of 0.002048, our results suggest that the model is not doing the best at explaining the variability in the response variable. Our low adjusted r-squared value also indicates that our model is not telling us very much about how much of the variability for the response variable is due to explanatory variables that have impact on the response variables. Our p-value of 0.01648 is less than our standard significance level of 0.05, which suggests that we can reject the null hypothesis and our model is significant. We can also observe that the predictor variable is significant here because its p-value is 0.016482, which is less than our standard significance level of 0.05.

Overall, the fourth and fifth simple linear regression models performed the best because these were the only models that proved to be significant. Still, there is room for the models to be better fits for our data-set since the adjusted r-squared and multiple r-squared values were low across all variations of our simple linear regression models. There also appears to be a trend where the simple linear regression models are significant when our predictor variables are significant and insignificant when our predictor variables are insignificant. 

## Data Splitting

Now, we are going to use Logistic Regression, LDA, QDA, and KNN with the best neighbor to find the highest accuracy to predict `Direction_Today`.
```{r}
set.seed(1000)
BTC_split <- initial_split(BTC_tidied)
BTC_train <- training(BTC_split)
BTC_test  <- testing(BTC_split)
```

## Logistic Regression Model
### Logistic Model Specification
```{r}
lr_spec <- logistic_reg() %>%
  set_engine("glm") %>%
  set_mode("classification")
lr_spec
```

### Model Formula

According to the EDA above, we choose `Lag1`, `Lag2`, `Lag3`, `Lag4`, and `Lag5` as predictors at the beginning.
```{r}
rec_allpreds <- recipe(Direction_Today ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5, data = BTC_train)
  # step_normalize(all_numeric_predictors())
rec_allpreds
```

### Workflow
```{r}
lr_wf1 <- workflow() %>%
  add_recipe(rec_allpreds) %>%
  add_model(lr_spec)
```
### Model Fitting
```{r}
lr_fit1 <- fit(lr_wf1, data = BTC_train)
lr_fit1
```

### Final Accuracy
```{r}
augment(lr_fit1, new_data = BTC_test) %>%
  accuracy(truth = Direction_Today, estimate = .pred_class) %>%
  mutate(description = "LR Model with Lag1 to Lag5") -> acc_lr_fit1
acc_lr_fit1
```
### Confusion Matrix
```{r}
augment(lr_fit1, new_data = BTC_test) %>%
  conf_mat(truth = Direction_Today, estimate = .pred_class) %>%
  autoplot(type = "heatmap")
```

## Coefficients Table

Only two predictors are significant, and we know that predictors with a high p-value cause an increase in variance without a corresponding decrease in bias so we just keeping `Lag1` and `Lag3` as model predictors.
```{r}
lr_coef <- lr_spec %>%
  fit(Direction_Today ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5, data = BTC_train)
lr_coef$fit %>% 
  tidy() %>%
  mutate(sign_level = ifelse(p.value < 0.05, "Yes", "No")) 
  # filter(sign_level == "Yes")
```

### Revised Model Formula

We are now only using Lag1 and Lag3 as regressors. 
```{r}
rec <- recipe(Direction_Today ~ Lag1 + Lag3, data = BTC_train) 
  # step_normalize(all_numeric_predictors())
rec 
```

### Revised Workflow
```{r}
lr_wf2 <- workflow() %>%
  add_recipe(rec) %>%
  add_model(lr_spec)
```

### Model Fitting
```{r}
lr_fit2 <- fit(lr_wf2, data = BTC_train)
lr_fit2
```

### Final Accuracy
```{r}
lr_fit2 <- fit(lr_wf2, data = BTC_train)
augment(lr_fit2, new_data = BTC_test) %>%
  accuracy(truth = Direction_Today, estimate = .pred_class) %>%
  mutate(description = "LR Model with Lag1 and Lag3") -> acc_lr_fit2
acc_lr_fit2
```

### Confusion Matricies

As shown in the confusion matrix, the logistic regression model operate the two Lag variables as predictors. The predictions are represented by the rows of the confusion matrix, while the ground truth is represented by the columns. The up-left area represents true Down, and the down-right area represents true Up; these two values are a leading indicator for determining whether the model is right or wrong. The true Down is 42, which greater then the false Down 39, and the true UP is 344, which is greater than false negative 277. Compared to the observed data points, the predictions donâ€™t work really well. The model just splitting approximately 50 % to the down and 50 % to the up direction.
```{r}
augment(lr_fit2, new_data = BTC_test) %>%
  conf_mat(truth = Direction_Today, estimate = .pred_class) %>%
  autoplot(type = "heatmap")
```

## Linear Discriminant Analysis Model
### Linear Discriminant Model Specification
```{r}
# set LDA specification
lda_spec <- discrim_linear() %>%
  set_mode("classification") %>%
  set_engine("MASS") # the default is "MASS", otherwise is "mda"
lda_spec
```

### Workflow
```{r}
lda_wf1 <- workflow() %>%
  add_recipe(rec) %>%
  add_model(lda_spec)
```

### Model Fitting
```{r}
lda_fit1 <- fit(lda_wf1, data = BTC_train)
lda_fit1
```

### Final Accuracy
```{r}
augment(lda_fit1, new_data = BTC_test) %>%
  accuracy(truth = Direction_Today, estimate = .pred_class) %>%
  mutate(description = "LDA Model with Lag1 and Lag3") -> acc_lda_fit1
acc_lda_fit1
```

### Confusion Matricies

The true Down is 40, which greater then the false Down 39, and the true UP is 344, which is greater than false negative 279 in LDA model.
```{r}
augment(lda_fit1, new_data = BTC_test) %>%
  conf_mat(truth = Direction_Today, estimate = .pred_class) %>%
  autoplot(type = "heatmap")
```

## Quadratic Discriminant Analysis Model
### Quadratic Discriminant Model Specification
```{r}
# set QDA specification
qda_spec <- discrim_regularized() %>%
  set_mode("classification") %>%
  set_args(frac_common_cov = 0, frac_identity = 0) %>%
  set_engine("klaR")
qda_spec
```

### Workflow
```{r}
qda_wf1 <- workflow() %>%
  add_recipe(rec) %>%
  add_model(qda_spec)
```

### Model Fitting
```{r}
qda_fit1 <- fit(qda_wf1, data = BTC_train)
qda_fit1
```

### Final Accuracy
```{r}
augment(qda_fit1, new_data = BTC_test) %>%
  accuracy(truth = Direction_Today, estimate = .pred_class) %>%
  mutate(description = "QDA Model with Lag1 and Lag3") -> acc_qda_fit1
acc_qda_fit1
```

### Confusion Matricies

The true Down is 188, which less then the false Down 203, and the true UP is 180, which is greater than false negative 131 in QDA model.
```{r}
augment(qda_fit1, new_data = BTC_test) %>%
  conf_mat(truth = Direction_Today, estimate = .pred_class) %>%
  autoplot(type = "heatmap")
```

## Cross Validation in KNN Model 
### Knn Model specification

We will use `neighbors = tune()` in order to find the optimal **K** in the KNN model.
```{r}
knn_spec <- nearest_neighbor(neighbors = tune()) %>%
  set_mode("classification") %>%
  set_engine("kknn")
knn_spec
```

### Centering and Scaling Formula

Since we are using a K-nearest neighbor model, it is importance that the variables are centered and scaled to make sure that the variables have a uniform influence. We can accomplish this transformation with `step_normalize()`, which does centering and scaling in one go.
```{r}
rec_norm <- recipe(Direction_Today ~ Lag1 + Lag3, data = BTC_train) %>%
  step_normalize(all_numeric_predictors())
rec_norm
```

### Workflow
```{r}
knn_wf1 <- workflow() %>%
  add_recipe(rec_norm) %>%
  add_model(knn_spec)
```
### V-Fold Cross Validation

Create the Cross-Validation term in order to use in the following `tune_grid()` session later, the number of default folds is 10.
```{r}
set.seed(1100)
cv10 <- vfold_cv(BTC_train, strata = Direction_Today, v = 10)
```

```{r}
# param_grid <- grid_regular(neighbors(), levels = 100)
param_grid <- tibble(neighbors = 1:100)
param_grid
```
### Tune the Best Neighbors
```{r, message=FALSE}
tune_res1 <- tune_grid(
  object = knn_wf1,
  resamples = cv10,
  grid = param_grid, control = control_grid(verbose = TRUE)
)
```

### Visualization

When the Nearest Neighbors value is 1, the accuracy and will be the best. The ROC curve has the best performance when the Nearest Neighbors value is 8.
```{r}
autoplot(tune_res1) +
  geom_vline(xintercept = 1, color = "red") +
  geom_vline(xintercept = 8, color = "blue")
```

```{r}
tune_res1 %>%
  show_best(metric = "accuracy")
tune_res1 %>%
  show_best(metric = "roc_auc")
```

### Fit the model

We will choose the best accuracy to fit the model. That is, use 1 as the k-nearest neighbors to fit the model.
```{r}
best_accuracy1 <- select_best(tune_res1, metric = "accuracy")
best_accuracy1
knn_final1 <- finalize_workflow(knn_wf1, best_accuracy1)
```

```{r}
knn_fit1 <- fit(knn_final1, data = BTC_train)
knn_fit1
```

### Final Accuracy
```{r}
augment(knn_fit1, new_data = BTC_test) %>%
  accuracy(truth = Direction_Today, estimate = .pred_class) %>%
  mutate(description = "KNN Model with Lag1 and Lag3 (CV)") -> acc_knn_fit1
acc_knn_fit1
```

### Confusion Matricies

The true Down is 148, which less then the false Down 173, and the true UP is 210, which is greater than false negative 170 in KNN with 1 neighbor model.
```{r}
augment(knn_fit1, new_data = BTC_test) %>%
  conf_mat(truth = Direction_Today, estimate = .pred_class) %>%
  autoplot(type = "heatmap")
```

## Bootstrapping in KNN Model
### Bootstrap Sampling

Use 25 `bootstraps` as the resamples data set.
```{r}
set.seed(1200)
boots25 <- bootstraps(BTC_train, strata = Direction_Today, times = 25)
```

### Tune the Best Neighbors
```{r, message=FALSE}
tune_res2 <- tune_grid(
  object = knn_wf1,
  resamples = boots25,
  grid = param_grid, control = control_grid(verbose = TRUE)
)
```

### Visualization

We can see the blue line that if the Nearest Neighbors is 1, the accuracy is the highest in bootstrap resampling. Thus, the best hyperparameter of this model might be 1.
```{r}
autoplot(tune_res2) +
  geom_vline(xintercept = 1, color = "red") +
  geom_vline(xintercept = 5, color = "blue")
```

```{r}
tune_res2 %>%
  show_best(metric = "accuracy")
tune_res2 %>%
  show_best(metric = "roc_auc")
```

### Fit the model

The best accuracy value to fit the KNN model will be chosen. That is, use 10 as the k-nearest neighbors to fit the model.
```{r}
best_accuracy2 <- select_best(tune_res2, metric = "accuracy")
best_accuracy2
knn_final2 <- finalize_workflow(knn_wf1, best_accuracy2)
```

```{r}
knn_fit2 <- fit(knn_final2, data = BTC_train)
knn_fit2
```

### Final Accuracy
```{r}
augment(knn_fit2, new_data = BTC_test) %>%
  accuracy(truth = Direction_Today, estimate = .pred_class) %>%
  mutate(description = "KNN Model with Lag1 and Lag3 (Bootstrap)") -> acc_knn_fit2
acc_knn_fit2
```

### Confusion Matricies

The true Down is 149, which less then the false Down 173, and the true UP is 210, which is greater than false negative 170 in KNN with 1 neighbor model. The outcome is the same as with the KNN model of Cross-Validation.
```{r}
augment(knn_fit2, new_data = BTC_test) %>%
  conf_mat(truth = Direction_Today, estimate = .pred_class) %>%
  autoplot(type = "heatmap")
```

# 8. Conclusion

Logistic regression model is the best predictor of up and down direction in Bitcoin. Simply providing `Lag1` and `Lag3` as predictors causes the model to correctly predict the trend in **55 percent ** of the cases.
```{r}
com_table <- bind_rows(acc_lr_fit1, acc_lr_fit2, acc_lda_fit1,
                       acc_qda_fit1, acc_knn_fit1, acc_knn_fit2)
com_table %>%
  arrange(desc(.estimate, everything()))
```

Furthermore, the ROC curve shows that the outcomes of these six models are not very different.
```{r}
models <- list("Logistic Regression 1" = lr_fit1,
               "Logistic Regression 2" = lr_fit2,
               "LDA" = lda_fit1,
               "QDA" = qda_fit1,
               "KNN CV" = knn_fit1,
               "KNN Boots" = knn_fit2
               )
preds <- imap_dfr(models, augment, 
                  new_data = BTC_test, .id = "model")

#preds %>%
 # select(model, Direction_Today, .pred_class, .pred_Down, .pred_Up)
#multi_metric <- metric_set(accuracy, sensitivity, specificity)
#preds %>%
 # group_by(model) %>%
  #multi_metric(truth = Direction_Today, estimate = .pred_class)
preds %>%
  group_by(model) %>%
  roc_curve(Direction_Today, .pred_Down) %>%
  autoplot() +
  labs(y = "Sensitivity (true positives/all actual positives)",
       x = "Specificity (true negatives/all actual negatives)") 
```

# 9. References
- James, G., Witten, D., Hastie, T., & Tibshirani, R. (2014).*An introduction to statistical learning: with applications in R / Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani.*(Corrected edition.). Springer.
- https://financeformulas.net/Total-Stock-Return.html
- https://www.tmwr.org/