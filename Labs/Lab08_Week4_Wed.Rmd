---
title: "Shrinkage and Hyperparameter Tuning"
author: "Yunting Chiu"
date: "`r Sys.Date()`"
output:
  html_document: 
    theme: journal
    highlight: haddock
---
```{r setup, include=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
This week we will talk about **shrinkage** and **hyperparameter** tuning.

We will use the `Hitters` data set from the `ISLR` library. It can be loaded using the following code

The vast majority of variables are numerical, with the remainder being factors.
```{r, message=FALSE}
library(tidyverse)
library(tidymodels)
library(ISLR)
data("Hitters")
Hitters %>%
  str()
```

Remove all rows where the salary is `NA` and split the data into testing and training data sets.
```{r}
Hitters %>%
  filter(!is.na(Salary)) -> Hitters_narm

set.seed(1234)
Hitters_split <- initial_split(Hitters_narm)
Hitters_train <- training(Hitters_split)
Hitters_test <- testing(Hitters_split)
```

# No Penalty in L2 regularization

a. Use `linear_reg()` with  `mixture = 0` to specify a **ridge regression model**.

- `mixture = 0` is L2 regularization. That is, ridge.
- `mixture = 1` is L1 regularization. That is, lasso.

Let's set the `penalty` to `0` to see what happens.
```{r}
ridge_spec0 <- linear_reg(mixture = 0, penalty = 0) %>%
  set_mode("regression") %>%
  set_engine("glmnet")

ridge_spec0
```

Put in our ingredients and get a recipe. Because ridge regression is scale sensitive, we must ensure that the variables are on the same scale by using `step_normalize(all_predictors())`.
```{r}
ridge_rec <- recipe(Salary ~ ., data = Hitters_train) %>%
  step_novel(all_nominal_predictors()) %>% # Novel Factor Levels
  step_dummy(all_nominal_predictors()) %>%
  step_zv(all_predictors()) %>% # remove variables that contain only a single value
  step_normalize(all_predictors()) # center and scale each column

ridge_rec
```

b. Fit the model on the data and inspect the model. What do you see?
```{r}
ridge_wf0 <- workflow() %>%
  add_model(ridge_spec0) %>%
  add_recipe(ridge_rec)
```

```{r}
ridge_fit0 <- fit(ridge_wf0, data = Hitters_train)
```

c. Try to predict using this model. What are your output?

If $\lambda = 0$ we don't have any penalization so we still get the standard OLS estimates. A high root mean squared error indicates that the model may predict the incorrect answer.
```{r}
augment(ridge_fit0, new_data = Hitters_test) %>%
  rmse(truth = Salary, estimate = .pred)
```

The plot shows we can try to optimize the penalty term. That is, tuning the hyperparameter in the ridge regression model.
```{r}
augment(ridge_fit0, new_data = Hitters_test) %>%
  ggplot(aes(Salary, .pred)) +
  geom_abline(slope = 1, intercept = 0) +
  geom_point() +
  theme_bw()
```

# Hyperparameter Tuning

d. Use {tune} to setup hyperparameter tuning to determine the right amount of regularization.

Now, we set `penalty = tune()` in order to find the best hyperparameter
```{r}
ridge_spec <- linear_reg(mixture = 0, penalty = tune()) %>%
  set_mode("regression") %>%
  set_engine("glmnet")
```

Look at the output below, the main arguments show `penalty = tune()`. Because the hyperparameter will be automatically turning.
```{r}
ridge_wf <- workflow() %>%
  add_model(ridge_spec) %>%
  add_recipe(ridge_rec)
ridge_wf
```

Create the Cross-Validation term in order to use in the following `tune_grid()` session, the number of default folds is 10.
```{r}
set.seed(123)
Hitters_fold <- vfold_cv(Hitters_train)
```

Regularly predict the penalty 100 times using regular grids, with the penalty range limited to $0$ to $10^5$. Note, these are in transformed units, the default transformation is $log10$.
```{r}
# penalty_grid <- grid_regular(list(p1 = threshold(), p2 = threshold()), levels = 10, size = 100)
# grid_max_entropy()
#penalty_grid %>%
  #ggplot(aes(p1, p2)) +
  # geom_point()
# grid_max_entropy: try to not have overlapping parts
penalty_grid <- grid_regular(penalty(range = c(0, 5)), levels = 100)
```

Show the penalty values with descending order
```{r}
penalty_grid %>%
  arrange(desc(penalty))
```

```{r}
tune_res <- tune_grid(
  object = ridge_wf, 
  resamples = Hitters_fold,
  grid = penalty_grid) # control = control_grid(verbose = TRUE): fitting model one by one
tune_res
```


Display the each penalty on rmse and rsq, respectively.
```{r}
tune_res %>%
  collect_metrics()
```
Display the best five rmse value of penalty
```{r}
tune_res %>%
  show_best(metric = "rmse")
```
# Visualize the RMSE and RSQ

We can see that if the amount of regularization is close to 1000, the rmse is low and the rsq is high, on average. Thus, the best hyperparameter of this model should be here.
```{r}
tune_res %>%
  autoplot()
```

The best rmse of the ridge regression model is 335.1603.
```{r}
best_rmse <- select_best(tune_res, metric = "rmse")
best_rmse
```
# Fit the Best Hyperparameter

e. Fit the best model. How does your parameter estimates look like?
```{r}
ridge_final <- finalize_workflow(ridge_wf, best_rmse)
```

```{r}
ridge_final_fit <- fit(ridge_final, data = Hitters_train) 
```

When we tuned the hyperparameter in the model, the rmse does not show a obvious reduction when compared to the `penalty = 0`'s rmse: 228. This means that tuning the hyperparameter will not work in this model.
```{r}
augment(ridge_final_fit, new_data = Hitters_test) %>%
  rmse(truth = Salary, estimate = .pred)
```

Because the data points are still separated, this method does not provide much benefit for weight loss.
```{r}
augment(ridge_final_fit, new_data = Hitters_test) %>%
  ggplot(aes(Salary, .pred)) +
  geom_abline(slope = 1, intercept = 0) +
  geom_point() +
  theme_bw()
```

# References
- https://emilhvitfeldt.github.io/ISLR-tidymodels-labs/linear-model-selection-and-regularization.html
- https://www.tmwr.org/grid-search.html
