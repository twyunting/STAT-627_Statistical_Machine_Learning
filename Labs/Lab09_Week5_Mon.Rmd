---
title: "Spline"
author: "Yunting Chiu"
date: "`r Sys.Date()`"
output:
  html_document: 
    theme: journal
    highlight: haddock
---
```{r setup, include=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
We will use the `ames` data set from the `modeldata` library. It can be loaded using the following code

# Ames Housing Data
```{r, message=FALSE}
library(tidymodels)
library(tidyverse)
data("ames")
ames
```

We will try to predict the `Sale_Price` of a house by the `Longitude` of its location (this would not be the best idea alone, but serves as an example). Use `step_bs()` to fit a spline onto `Longitude` use cross-validation to find the value of `degree` where the model performs best.

# Data Visualization

According to the true underlying relationship between `Longitude` and `Sale_Price` , The plot does not follow the linear trend.
```{r}
ggplot(ames, aes(Longitude, Sale_Price)) +
  geom_point() +
  theme_bw() +
  geom_smooth(se = FALSE)
```

# step_bs()
To begin with, we will split `ames` into training and testing sets.
```{r}
set.seed(1234)
ames_split <- initial_split(ames)
ames_train <- training(ames_split)
ames_test  <- testing(ames_split)
```

Construct linear regression model specification, basis spline recipe, and basis spline workflow
```{r}
lm_spec <- linear_reg() %>%
  set_mode("regression") %>%
  set_engine("lm")

rec_bs <- recipe(Sale_Price ~ Longitude, ames_train) %>%
  step_bs(Longitude, degree = tune())

wf_bs <- workflow() %>%
  add_model(lm_spec) %>%
  add_recipe(rec_bs)
```

create 10-Fold Cross-Validation in the training data set.
```{r}
set.seed(4321)
ames_folds <- vfold_cv(ames_train, strata = Sale_Price)
ames_folds
```


```{r}
# param_gris <- grid_regular(degree_int(range = c(1, 5)), levels = 5)
```

Make a data frame with values ranging from 1 to 10. Later, we will piecewise fit the best multiple polynomials.
```{r}
param_grid <- tibble(degree = 1:10)
#param_grid <- grid_regular(degree_int(range = c(1, 10)), levels = 20)
param_grid
```

```{r, warning=FALSE, message=FALSE}
tune_res <- tune_grid(
  object = wf_bs,
  resamples = ames_folds,
  grid = param_grid, control = control_grid(verbose = TRUE)
)
# tune_res$.notes[[1]]
```

Our goal is find the lowest value of rmse and the highest value of rsq. Look at the plot; if the spline degree is 10, the rmse and rsq performance will be the best. 
```{r}
tune_res %>%
  collect_metrics()
autoplot(tune_res) +
  geom_vline(xintercept = 10, color = "red") 
```

```{r}
tune_res %>%
  show_best(metric = "rmse")
tune_res %>%
  show_best(metric = "rsq")
```

We will take the best root mean squared error solution to fit the model. That is, **the spline degree is 10**.
```{r}
final_wf_bs <- finalize_workflow(wf_bs, select_best(tune_res, metric = "rmse"))
```

```{r}
final_fit_bs <- fit(final_wf_bs, data = ames_train)
tidy(final_fit_bs)
```

```{r}
augment(final_fit_bs, new_data = ames_test) %>%
  rmse(truth = Sale_Price, estimate = .pred) %>%
  mutate(note = "bs model") -> bs_model
bs_model
```


Next we will use `step_discretize()` and `step_cut()` to fit step function into `Longitude` to see if that works better.

# step_discretize()
```{r}
rec_discretize <- recipe(Sale_Price ~ Longitude, ames_train) %>%
  step_discretize(Longitude, num_breaks = tune())
```

```{r}
wf_discretize <- workflow() %>%
  add_model(lm_spec) %>%
  add_recipe(rec_discretize)
```

```{r}
param_grid2 <- tibble(num_breaks = 1:10)
#param_grid <- grid_regular(degree_int(range = c(1, 10)), levels = 20)
param_grid2
```

```{r, warning=FALSE, message=FALSE}
tune_res2 <- tune_grid(
  object = wf_discretize,
  resamples = ames_folds,
  grid = param_grid2, control = control_grid(verbose = TRUE)
)

# tune_res2$.notes # Error: There should be at least 2 cuts
```

We will take the best root mean squared error solution to fit the model. That is, **the spline degree is 8**.
```{r}
tune_res2 %>%
  collect_metrics()
autoplot(tune_res2) +
  geom_vline(xintercept = 8, color = "red") 
```

```{r}
tune_res2 %>%
  show_best(metric = "rmse")
tune_res2 %>%
  show_best(metric = "rsq")
```

```{r}
final_wf_discretize <- finalize_workflow(wf_discretize, select_best(tune_res2, metric = "rmse"))
```

```{r}
final_fit_discretize <- fit(final_wf_discretize, data = ames_train)
tidy(final_fit_discretize)
```

```{r}
augment(final_fit_discretize, new_data = ames_test) %>%
  rmse(truth = Sale_Price, estimate = .pred) %>%
  mutate(note = "discretize model") -> discretize_model
discretize_model
```

# step_cut()

Now, we can supply the breaks **manually**.
```{r}
rec_cut <- recipe(Sale_Price ~ Longitude, ames_train) %>%
  step_cut(Longitude, breaks = c(-93.675, -93.650, -93.625))

#ames_train %>%
  #select(Longitude) %>%
 # arrange(desc(Longitude)) %>%
 # tail()
```

```{r}
wf_cut <- workflow() %>%
  add_model(lm_spec) %>%
  add_recipe(rec_cut)
```

```{r}
final_fit_cut <- fit(wf_cut, data = ames_train)
tidy(final_fit_cut)
```

```{r}
augment(final_fit_cut, new_data = ames_test) %>%
  rmse(truth = Sale_Price, estimate = .pred) %>%
  mutate(note = "cut model") -> cut_model
cut_model
```
# Comparison

The discretize model has the best performance in predicting `Sale_Price` based on `Longitude` because it has the smallest rmse.
```{r}
bind_rows(bs_model, discretize_model, cut_model)
```

