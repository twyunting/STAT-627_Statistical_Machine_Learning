---
title: "Labs 03 - Week 2 Monday"
author: "Yunting Chiu"
date: "`r Sys.Date()`"
output:
  html_document: 
    theme: lumen
---

```{r setup, include=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
These sets of labs will introduce you to logistic regression. This will also be your first introduction to the [rsample](https://rsample.tidymodels.org/) package which we will use to perform train-test split.

# Exercise 1
In this exercise we will explore the `mlc_churn` data set included in **tidymodels**.

```{r}
library(tidyverse)
library(tidymodels)
data("mlc_churn")

# know the Customer churn data in advance
help("mlc_churn")
```
The data set contains a variable called `churn`

- We will be modeling customer churn. Before we go over the questions, let's take a look at what's going on in the response variable `churn`. In the graph below, we can see that many customers gave negative feedback rather than positive feedback.
```{r}
mlc_churn %>%
  count(churn) %>%
  ggplot(aes(churn, n)) +
  geom_col()
```

a. Create a test-train `rsplit` object of `mlc_churn` using `initial_split()`. Use the arguments to set the proportions of the training data to be 80%. Stratify the sampling according to the `churn` variable. How many observations are in the testing and training sets?

- Ideally, we would divide the data into 75-80% for training and 25%-30% for testing. We are going to take 80 % training data and take 20 % testing data in this exercise.
- There are 4001 observations in the training sets, and 999 observations in the testing sets. Totally the data frame has 5000 observations.
```{r}
set.seed(1) # Ensure that the data can be samely separated into the same observations.
mlc_split <- initial_split(mlc_churn, prop = 0.8, strata = churn)
mlc_split
```

b. Create the training and testing data set with `training()` and `testing()` respectively. Does the observation counts match what you found in the last question?

- Using `nrow()`, we can exactly see the traning and testing sets have 4001 and 999, respectively.
```{r}
mlc_training <- training(mlc_split)
nrow(mlc_training)
mlc_testing <- testing(mlc_split)
nrow(mlc_testing)
```

c. Fit a logistic regression model using logistic_reg(). Use number_vmail_messages, total_intl_minutes, total_intl_calls, total_intl_charge, number_customer_service_calls as predictors. Remember to fit the model only using the training data set.
```{r}
mlc_formula <- churn ~ number_vmail_messages + total_intl_minutes + total_intl_calls + total_intl_charge + number_customer_service_calls

# set a engine
lr_spec <- logistic_reg() %>%
  set_engine("glm") %>%
  set_mode("classification")

lr_spec %>%
  fit(mlc_formula, data = mlc_training) -> lr_fit

lr_fit
```

d. Inspect the model with summary() and tidy(). How good are the variables we have chosen?
```{r}
tidy(lr_fit)

mlc_training %>%
  count(churn)
```
```{r}
lr_fit %>%
  pluck("fit") %>%
  summary()
```

```{r}
mlc_training %>%
  ggplot(aes(total_intl_minutes, total_intl_charge)) +
  geom_point()
```

e. Predict values for the testing data set. Use the type argument to also get probability predictions.
```{r}
# predict(lr_fit, new_data = mlc_testing)
preds <- augment(lr_fit, new_data = mlc_testing)
preds
```


f. Use `conf_mat()` to construct a confusion matrix (error matrix). Does the confusion matrix look good?
```{r}
preds %>%
  mutate(.pred_class_40 = ifelse(.pred_no > 0.4, 'no', 'yes')) %>%
  conf_mat(estimate = .pred_class_40, truth = churn) %>%
  autoplot(type = "heatmap")
```

`conf_mat()` is used as follows, where truth is the name of the true response variable and estimate is the name of the predicted response.

```{r}
#data %>%
  #conf_mat(truth, estimate)
```


