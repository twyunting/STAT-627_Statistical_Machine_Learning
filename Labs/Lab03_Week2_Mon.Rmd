---
title: "Lab 03 - Week 2 Monday"
author: "Yunting Chiu"
date: "`r Sys.Date()`"
output:
  html_document: 
    theme: cerulean
---

```{r setup, include=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
These sets of labs will introduce you to logistic regression. This will also be your first introduction to the [rsample](https://rsample.tidymodels.org/) package which we will use to perform train-test split.

# Exercise 1
In this exercise we will explore the `mlc_churn` data set included in **tidymodels**.

```{r, warning=FALSE}
suppressPackageStartupMessages(library(tidyverse))
suppressPackageStartupMessages(library(tidymodels))
data("mlc_churn")

# know the Customer churn data in advance
help("mlc_churn")
```
The data set contains a variable called `churn`

- We will be modeling customer churn. Before we go over the questions, let's take a look at what's going on in the response variable `churn`. In the graph below, we can see that many customers gave negative feedback rather than positive feedback.
```{r}
mlc_churn %>%
  count(churn) %>%
  ggplot(aes(churn, n)) +
  geom_col()
```

a. Create a test-train `rsplit` object of `mlc_churn` using `initial_split()`. Use the arguments to set the proportions of the training data to be 80%. Stratify the sampling according to the `churn` variable. How many observations are in the testing and training sets?

- Ideally, we would divide the data into 75-80% for training and 25%-30% for testing. We are going to take 80 % training data and take 20 % testing data in this exercise.
- There are 4001 observations in the training sets, and 999 observations in the testing sets. Totally the data frame has 5000 observations.
```{r}
set.seed(1) # Ensure that the data can be samely separated into the same observations.
mlc_split <- initial_split(mlc_churn, prop = 0.8, strata = churn)
mlc_split
```

b. Create the training and testing data set with `training()` and `testing()` respectively. Does the observation counts match what you found in the last question?

- Using `nrow()`, we can exactly see the training and testing sets have 4001 and 999, respectively.
```{r}
mlc_training <- training(mlc_split)
nrow(mlc_training)
mlc_testing <- testing(mlc_split)
nrow(mlc_testing)
```

c. Fit a logistic regression model using `logistic_reg()`. Use `number_vmail_messages, total_intl_minutes, total_intl_calls, total_intl_charge, number_customer_service_calls` as predictors. Remember to fit the model only using the training data set.
```{r}
mlc_formula <- churn ~ number_vmail_messages + total_intl_minutes + total_intl_calls + total_intl_charge + number_customer_service_calls

# set a engine as logistic regression
lr_spec <- logistic_reg() %>%
  set_engine("glm") %>%
  set_mode("classification")

lr_spec %>%
  fit(mlc_formula, data = mlc_training) -> lr_fit
```

d. Inspect the model with `summary()` and `tidy()`. How good are the variables we have chosen?

It can be seen that only 3 out of the 5 predictors are significantly associated to the outcome. With the small p-value, the significant predictors are `number_vmail_messages`, `total_intl_calls`, and `number_customer_service_calls`. The logistic regression coefficients represent the change in the log odds of the outcome for each unit increase in the predictor variable.

```{r}
# using summary()
lr_fit %>%
  pluck("fit") %>% # unest the list
  summary()
```
Note: 

1. Null deviance: It shows how well the response variable is predicted by a model that includes only intercept.
2. Residual deviance: It shows how well the response variable is predicted by a model that include all independent variables.
3. AIC: AIC provides a method for assessing the quality of your model through comparison of related models. Small value of AIC is better.

```{r}
# using tidy()
tidy(lr_fit)
```
## Reference
- https://www.excelr.com/blog/data-science/regression/understanding-logistic-regression-using-r
- https://stats.idre.ucla.edu/r/dae/logit-regression/
- http://www.sthda.com/english/articles/36-classification-methods-essentials/151-logistic-regression-essentials-in-r

e. Predict values for the testing data set. Use the type argument to also get probability predictions.

- The simplest way to predict based on the data on which it was trained. The default column type of `.pred_class` is factor.
```{r}
predict(lr_fit, new_data = mlc_testing) %>% # using reg model and testing dataset for testing 
  head()
```
- We can also get probability predictions by using `type = "prob"` in `predict()`.
```{r}
predict(lr_fit, new_data = mlc_testing, type = "prob") %>%
  head()
```
- `augment()` can automatically add the `.pred_class` and the probability of yes and no, and add in the existing data frame as new columns. 
```{r}
preds <- augment(lr_fit, new_data = mlc_testing)
preds %>%
  names()
```

f. Use `conf_mat()` to construct a confusion matrix (error matrix). Does the confusion matrix look good?

- The up-left area represents true negative, and the down-right area represents true positive; these two values are a leading indicator for determining whether the model is good or not. The true negative is 10, which greater then the false positive 2, and the true positive is 856, which is greater than false negative 131. Thus, the confusion matrix shows the model has a good prediction. 
```{r}
preds %>%
  conf_mat(truth = churn, estimate = .pred_class) -> confusionMatrix
confusionMatrix

# we can also plot the confusion matrix
confusionMatrix %>%
  autoplot(type = "heatmap")
```

- The `accuracy()`is useful for demonstrating the correct percentage of model prediction. So we know the logistic regression model has 87 % accuracy for predicting the customer churn, which can persuade that is a good model again.
```{r}
preds %>%
accuracy(truth = churn, estimate = .pred_class)
```
```{r}
# preds %>%
#   mutate(.pred_class_40 = ifelse(.pred_no > 0.4, 'no', 'yes')) %>%
#   mutate(.pred_class_40 = as.factor(.pred_class_40)) %>%
#   conf_mat(estimate = .pred_class_40, truth = churn) %>%
#   autoplot(type = "heatmap")
```
Note: `conf_mat()` is used as follows, where truth is the name of the true response variable and estimate is the name of the predicted response.



